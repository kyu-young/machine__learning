{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상환경 만들기 conda create -n data_env_tensorflow2 python=3.7 openssl\n",
    "# 필요한 모듈 설치\n",
    "# 텐서플로우 버전 확인해보기\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)  # 현재 설치된 TF 버전을 출력!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 랜덤값을 하나 읽어와요!\n",
    "# numpy\n",
    "random1 = np.random.rand(2,2)\n",
    "print(random1)\n",
    "\n",
    "# tensorflow\n",
    "random2 = tf.random.normal([1], dtype=tf.float32)\n",
    "print(random2)   # tesor가 출력되요!\n",
    "# tf 1.x 버전에서는 node가 가지는 값을 얻어오려면 (node를 실행시키려면)\n",
    "# Session이 있어야 했어요!\n",
    "# tf 2.x 버전에서는 session없이 즉시 실행시킬 수 있어요!!(Eager Excution)\n",
    "print(random2.numpy())\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(10, dtype=tf.float32)\n",
    "b = tf.constant(20, dtype=tf.float32)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "print('c의 값은 : {}'.format(c.numpy()))\n",
    "\n",
    "d = 30.0\n",
    "\n",
    "tensor_d = tf.convert_to_tensor(d)\n",
    "\n",
    "print((c + tensor_d).numpy())\n",
    "import tensorflow as tf\n",
    "W = tf.Variable(tf.random.normal([1]),name ='weight')\n",
    "\n",
    "# 기존에는 tf.Variable()을 이용해서 변수를 만들면 사용하기 전에\n",
    "# 반드시 초기화를 진행해야 했어요!\n",
    "# sess.run(tf.global_variables_initializer()) \n",
    "# TF 2.0에서는 초기화를 안해도 되요!!\n",
    "print(W.numpy())\n",
    "# tensorflow graph에 입력을 주는 부분이 없어졌어요!\n",
    "# 기존에는 graph에게 데이터를 밀어넣기 위해서 placeholder를 이용했어요!\n",
    "# Eager Excution에 의해서 이제는 placeholder가 필요 없게 됐어요!\n",
    "# placsholder는 삭제되었습니다~\n",
    "\n",
    "# Lazy excution을 하지않아요! 이젠 Eager Excution을 수행해요!\n",
    "# tensorflow의 keras를 이용하여 Model을 생성해 보아요!\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# model = Sequential()  # keras model 생성\n",
    "\n",
    "model = tf.keras.models.Sequential()   # keras model 생성\n",
    "\n",
    "# model을 만들었으니 그 다음에는 layer를 만들어야 해요!\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(2,))) \n",
    "# input layer를 추가\n",
    "# 독립변수 2개 짜리 input layer\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "# output layer 추가\n",
    "# Dense(3) : 결과값의 개수\n",
    "# softmax를 이용해 결과값을 확률값으로 바꿔서\n",
    "# t를 onehot인코딩한 값과 비교!\n",
    "\n",
    "# modelcompile 과정\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# sklearn은 이미 만들어진 기성복같은!! 파라미터만 정하는것!\n",
    "# keras는 내가 원하는 방식으로 로직 자체를 바꿀수 있디!(모듈화!!)\n",
    "def my_loss:\n",
    "    pass\n",
    "\n",
    "# model 학습\n",
    "model.fit(x_data_train,\n",
    "          t_data_train,\n",
    "          epochs=100,\n",
    "          batch_size=100,\n",
    "          validation_split=0.3)\n",
    "# tensorflow 2.1을 이용해서 ozone 예제를 다시 구현해보아요!\n",
    "# Multiple Linear Regression\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential  # Model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Layer\n",
    "from tensorflow.keras.optimizers import SGD   # Optimizer\n",
    "from sklearn.preprocessing import MinMaxScaler   # Normalization\n",
    "from scipy import stats   # 이상치처리\n",
    "\n",
    "# Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "\n",
    "# 결측치 확인 및 처리\n",
    "# 일단 데이터가 충분히 많고 결측치가 적으면 삭제가 답이에요!\n",
    "# 하지만 일반적으로 결측치를 삭제하면 데이터가 너무 많이 유실되기 때문에\n",
    "# 다른 방식을 이용하게 되요!\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())  # 각 컬럼별 결측치 개수\n",
    "# KNN의 사용법에 대해서 알아보아요!\n",
    "# sklearn을 이용해서 알아보아요!\n",
    "\n",
    "# BMI예제를 이용해서 학습한 후 정확도를 특정해 보아요!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/bmi.csv')\n",
    "\n",
    "# data split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df[['height', 'weight']],\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                random_state=0)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_data_train_norm, t_data_train)\n",
    "print(model.score(x_data_test_norm, t_data_test)) # 생성된 모델 성능 확인\n",
    "\n",
    "# KNN을 이용한 분류\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(x_data_train_norm, t_data_train)\n",
    "print(knn_model.score(x_data_test_norm, t_data_test))   # 생성된 모델 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn의 결과 : [[38.75927452]]\n",
      "tensorflow의 결과 : [[38.626904]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats # 이상치 처리를 위해서 필요\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "import warnings\n",
    "\n",
    "# KNeighborsRegressor  : Regression 처리 할떄 사용 , 값을 찾을 때 사용\n",
    "# KNeighborsClassifier : 분류할 떄 사용 , 0 과 1 등등\n",
    "warnings.filterwarnings(action='ignore') # warning 출력을 하지 않아요\n",
    "\n",
    "# Raw Data Loading\n",
    "\n",
    "df = pd.read_csv('/Users/mac/notebook_dir/data/ozone.csv')\n",
    "\n",
    "training_data = df\n",
    "\n",
    "x_data = training_data[['Solar.R','Wind','Temp']]\n",
    "t_data = training_data['Ozone']\n",
    "\n",
    "# split을 하지 않는 이유는 linear regression을 하는거기 때문에 \n",
    "# 정확도 측정이 의미가 없기에 하지 않는다.\n",
    "\n",
    "# 결측치 확인\n",
    "# 1. 독립변수에 대한 결측치 처리부터 할게요 !\n",
    "# df.isnull().sum()\n",
    "# Ozone 이 종속변수, 나머지가 독립변수\n",
    "#   Solar.R 에 7개의 결측치가 있는데 median으로 처리할게요 !\n",
    "# nanmedian : nan을 제외하고 나머지 데이터의 데해 median을 구해라\n",
    "for col in x_data.columns:\n",
    "    col_median = np.nanmedian(x_data[col])\n",
    "    x_data[col].loc[x_data[col].isnull()] = col_median\n",
    "    \n",
    "# x_data.isnull().sum()\n",
    "\n",
    "# 2. 독립변수에 대한 이상치를 검출한 후 이 이상치는 mean 처리할게요 \n",
    "zscore = 1.8\n",
    "\n",
    "for col in x_data.columns:\n",
    "    outliers = x_data[col][np.abs(stats.zscore(x_data[col])) > zscore]\n",
    "#     print(outliers)\n",
    "    col_mean = np.mean(x_data.loc[~x_data[col].isin(outliers),col])\n",
    "    x_data.loc[x_data[col].isin(outliers),col] = col_mean\n",
    "#     print(col_mean)\n",
    "\n",
    "\n",
    "# 3. 정규화 진행해요 !\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_t = MinMaxScaler()\n",
    "\n",
    "# MinMax scaling하려면 2차원이 들어가야 하기 때문에 reshape 해줘야함\n",
    "scaler_x.fit(x_data.values)\n",
    "scaler_t.fit(t_data.values.reshape(-1,1))\n",
    "\n",
    "x_data_norm = scaler_x.transform(x_data)  # 2차원\n",
    "t_data_norm = scaler_t.transform(t_data.values.reshape(-1,1)).ravel()\n",
    "# 2차원으로 변경\n",
    "# 나중에 1차원으로 사용되야야 해서 그래서 ravel() 사용.\n",
    "    \n",
    "\n",
    "# 4. 종속변수(Ozone)에 대한  결측치는 KNN을 이용해서 예측값으로 imputation할 거에요 !\n",
    "#    학습에 사용될 x_data_train_norm , t_data_train_norm을 구해야 해요 !\n",
    "\n",
    "x_data_train_norm = x_data_norm[~np.isnan(t_data_norm)]\n",
    "t_data_train_norm = t_data_norm[~np.isnan(t_data_norm)]\n",
    "\n",
    "# KNN 모델 생성후 학습진행\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=2)\n",
    "knn_regressor.fit(x_data_train_norm,t_data_train_norm)\n",
    "\n",
    "# knn_predict \n",
    "knn_predict = knn_regressor.predict(x_data_norm[np.isnan(t_data_norm)])\n",
    "t_data_norm[np.isnan(t_data_norm)] = knn_predict\n",
    "\n",
    "# 최종 데이터를 생성했어요 !\n",
    "# x_data_norm\n",
    "# t_data_norm\n",
    "\n",
    "###############################################\n",
    "# 학습을 진행해 보아요 ! Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "test_data = [[310, 15, 80]] # 테스트 데이터 !! Ozone량을 예측해 보아요 !\n",
    "\n",
    "# sklearn\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(x_data_norm,t_data_norm)\n",
    "result = sklearn_model.predict(scaler_x.transform(test_data)).reshape(-1,1)\n",
    "\n",
    "scaled_result = scaler_t.inverse_transform(result)\n",
    "print('sklearn의 결과 : {}'.format(scaled_result))\n",
    "\n",
    "# Tensorflow 2.x \n",
    "# 모델생성\n",
    "keras_model = Sequential()\n",
    "# 레이어 추가\n",
    "keras_model.add(Flatten(input_shape=(3,))) # 안에 독립변수의 개수를 shape로 잡아줘야함\n",
    "# input Layer\n",
    "keras_model.add(Dense(1, activation='linear'))\n",
    "# 아웃풋의 개수가 들어감 , Ozone값 하나가 아웃풋임\n",
    "# W는 레이어 중간에 위치함, 하지만 우리는 코드상에 W를 표시하지는 않음\n",
    "# bias는 dense layer안에 숨어있다. ( 노드 안에 있음 )\n",
    "\n",
    "# compile\n",
    "keras_model.compile(optimizer=SGD(learning_rate=1e-2),\n",
    "                   loss = 'mse') # mse : 최소제곱법\n",
    "# 학습\n",
    "keras_model.fit(x_data_norm, t_data_norm,\n",
    "               epochs=5000,\n",
    "               verbose=0) # verbose : accuracy 있을떄 쓰면됨\n",
    "# predict\n",
    "result = keras_model.predict(scaler_x.transform(test_data)).reshape(-1,1)\n",
    "\n",
    "scaled_result = scaler_t.inverse_transform(result)\n",
    "print('tensorflow의 결과 : {}'.format(scaled_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression에 대해서 sklearn과 Tensorflow 2.x 구현을\n",
    "## 해볼거에요 !!\n",
    "## titanic(kaggle) => logistic 문제 (결측치가 다수 있어요 !)\n",
    "## feature engineering\n",
    "\n",
    "## 데이터를 완전히 준비하는 건 여러분들이 해보세요 !\n",
    "## 데이터 준비가 끝나면 같이 구현하는 부분만 해 보아요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex  Age  Embarked  Family\n",
       "0           0       3    0  2.0         0       1\n",
       "1           1       1    1  2.0         1       1\n",
       "2           1       3    1  2.0         0       0\n",
       "3           1       1    1  2.0         0       1\n",
       "4           0       3    0  2.0         0       0\n",
       "..        ...     ...  ...  ...       ...     ...\n",
       "886         0       2    0  2.0         0       0\n",
       "887         1       1    1  1.0         0       0\n",
       "888         0       3    1  2.0         0       3\n",
       "889         1       1    0  2.0         1       0\n",
       "890         0       3    0  2.0         2       0\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats # 이상치 처리를 위해서 필요\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "train = pd.read_csv('/Users/mac/notebook_dir/data/titanic/train.csv')\n",
    "test = pd.read_csv('/Users/mac/notebook_dir/data/titanic/test.csv')\n",
    "\n",
    "train.drop(['PassengerId','Ticket','Cabin','Name','Fare'],axis = 1, inplace = True)\n",
    "\n",
    "# 성별처리\n",
    "\n",
    "sex_mapping = { 'male' : 0, 'female' : 1}\n",
    "train['Sex'] = train['Sex'].map(sex_mapping)\n",
    "\n",
    "# 가족처리\n",
    "train['Family'] = train['SibSp'] + train['Parch']\n",
    "train.drop(['SibSp','Parch'], axis = 1, inplace = True)\n",
    "\n",
    "# Embarked 결측치 처리\n",
    "\n",
    "train['Embarked'] = train['Embarked'].fillna('Q')\n",
    "\n",
    "# Age에 대한 결측치 처리\n",
    "\n",
    "train['Age'] = train['Age'].fillna(train['Age'].mean())\n",
    "\n",
    "# Embarked 문자 => 숫자 처리\n",
    "\n",
    "embarked_mapping = {'S' : 0 , 'C' : 1, 'Q' : 2}\n",
    "train['Embarked'] = train['Embarked'].map(embarked_mapping)\n",
    "\n",
    "# Age에 대해서 Binning 처리(Numerical value -> categorical value)\n",
    "train.loc[train['Age'] < 8, 'Age'] = 0 # 아이는 0\n",
    "train.loc[(train['Age'] >= 8) & (train['Age'] < 20), 'Age'] = 1\n",
    "train.loc[(train['Age'] >= 20) & (train['Age'] < 65), 'Age'] = 2\n",
    "train.loc[train['Age'] >= 65, 'Age'] = 3\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? ㅛ\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c0cd607cc8d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# Gate 연산으로 수행하는 Deep Learning으로 구현해보아요\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Training Data set\n",
    "x_data = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]], dtype = np.float32)\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# placeholder \n",
    "\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2,100]), name='weight2') \n",
    "# 몇개의 아웃풋이있나요?\n",
    "# 로지스틱이 몇개있나요? \n",
    "b2 = tf.Variable(tf.random.normal([100]), name='bias2')\n",
    "# 로지스틱이 100개있으니 \n",
    "# bias도 100개\n",
    "# shape만 변경해주면 됨 . 행렬 연산의 장점이다.\n",
    "layer2 = tf.sigmoid(tf.matmul(X, W2) + b2) \n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([100,6]), name='weight3') \n",
    "b3 = tf.Variable(tf.random.normal([6]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "# 지금까지는 히든 레이어는 2개\n",
    "\n",
    "\n",
    "\n",
    "# 최종 layer\n",
    "W4 = tf.Variable(tf.random.normal([6,1]), name='weight4') \n",
    "b4 = tf.Variable(tf.random.normal([1]), name='bias4')\n",
    "\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                             labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess  = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X:x_data,\n",
    "                                                    T:t_data})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss : {}'.format(loss_val))\n",
    "        \n",
    "# 성능평가 ( Accuracy )\n",
    "accuracy= tf.cast(H >= 0.5, dtype = tf.float32)\n",
    "result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "print(classification_report(t_data.ravel(), result.ravel()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
