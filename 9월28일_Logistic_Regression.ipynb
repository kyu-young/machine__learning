{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9월 28일 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변수 수치미분코드\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있는 numpy array(차원상관없음)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 계산된 수치미분 값을 저장하기 위한 변수\n",
    "    \n",
    "    # iterator를 이용하여 입력변수 x에 대해 편미분 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 index를 추출 => tuple형태로 리턴\n",
    "        \n",
    "        tmp = x[idx]           # 현재 idx의 값을 잠시 보존. delta_x를 이용한 값으로\n",
    "                               # ndarray를 수정한 후 함수값을 계산해야 하기 때문\n",
    "                               # 함수값을 계산한 후 원상복구해야 다음 변수에 대한 편미분을\n",
    "                               # 정상적으로 수행할 수 있다.\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)   # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "    \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### python 결과값 #########\n",
      "공부시간 : [[13]], 결과 : (1, array([[0.58065201]]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Logistic Regression을 python, tensorflow, sklearn으로 각각구현해 보아요!\n",
    "# 처음은 독립변수가 1개인 걸로 가요!!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 수치미분함수를 들고와서 사용해요!\n",
    "# ##################################\n",
    "# 다변수 수치미분코드\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있는 numpy array(차원상관없음)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 계산된 수치미분 값을 저장하기 위한 변수\n",
    "    \n",
    "    # iterator를 이용하여 입력변수 x에 대해 편미분 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 index를 추출 => tuple형태로 리턴\n",
    "        \n",
    "        tmp = x[idx]           # 현재 idx의 값을 잠시 보존. delta_x를 이용한 값으로\n",
    "                               # ndarray를 수정한 후 함수값을 계산해야 하기 때문\n",
    "                               # 함수값을 계산한 후 원상복구해야 다음 변수에 대한 편미분을\n",
    "                               # 정상적으로 수행할 수 있다.\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)   # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "    \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "# ##################################\n",
    "\n",
    "# Raw Data Loading + Data Preprocessing\n",
    "# 그런데 이번예제는 이 과정이 필요없죠!!\n",
    "\n",
    "# Training Data Set\n",
    "# 지도학습을 하고 있기 때문에 독립변수와 종속변수(label)로 구분해서 데이터를 준비\n",
    "# 어떤경우에는 이 두개를 아예 분리해서 제공하는 경우도 있어요!\n",
    "x_data = np.arange(2,21,2).reshape(-1,1)\n",
    "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)\n",
    "\n",
    "#########################################################\n",
    "# python 구현부터 해 보아요!!\n",
    "\n",
    "# Weight & bias     \n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)   # (1,)\n",
    "\n",
    "# 위에서 정의한 W와 b의 값을 구해야 해요!\n",
    "# 이 값만 구하면 우리의 최종 목적인 model을 완성할 수 있어요!\n",
    "\n",
    "# loss function(손실함수, cost function , 비용함수)\n",
    "# 우리 모델의 예측값과 들어온 t_data(정답)\n",
    "# 입력으로 들어온 x_data와 W,b값을 이용해서 예측값 계산\n",
    "# t_data(정답)을 비교해되요!!\n",
    "def loss_func(input_obj):\n",
    "    \n",
    "    # input_obj : W와 b를 같이 포함하고 있는 ndarray => [W1 W2 W3 b]\n",
    "    num_of_bias = b.shape[0]   # num_of_bias : 1\n",
    "    \n",
    "    input_W = input_obj[:-1*num_of_bias].reshape(-1,num_of_bias)   # 행렬연산을 하기 위한 W를 생성\n",
    "    input_b = input_obj[-1*num_of_bias:]                           # bias\n",
    "    \n",
    "    \n",
    "    #  우리 모델의 예측값 : (linear regression model(Wx + b) ==> sigmoid를 적용 )\n",
    "    z = np.dot(x_data,input_W) + input_b\n",
    "    y = 1 / ( 1 + np.exp(-1 * z) )  # sigmoid\n",
    "    \n",
    "    delta = 1e-7  #  굉장히 작은값을 이용해서 프로그램으로 \n",
    "                  # 로그 연산시 무한대로 발산하는것을 방지\n",
    "        \n",
    "    # cross entropy\n",
    "    return -np.sum(t_data*np.log(y+delta) + ((1-t_data)*np.log(1-y+delta)))\n",
    "    \n",
    "# learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    \n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()),axis=0)   # [W1 W2 W3 b]\n",
    "    derivative_result = learning_rate* numerical_derivative(loss_func,input_param)\n",
    "\n",
    "    num_of_bias = b.shape[0] \n",
    "    \n",
    "    W = W - derivative_result[:-1*num_of_bias].reshape(-1,num_of_bias)   # [[W1] [W2] [W3]]\n",
    "    b = b - derivative_result[-1*num_of_bias:]\n",
    "    \n",
    "    \n",
    "# predict => W,b를 다 구해서!! 우리의 Logistic Regression Model을 완성!!\n",
    "def logistic_predict(x):  # 공부한 시간이 입력으로 들어와요!!\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / ( 1 + np.exp(-1*z) )\n",
    "    \n",
    "    if y < 0.5:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "        \n",
    "    return result, y\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "result = logistic_predict(study_hour)\n",
    "print('####### python 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### sklearn 결과값 #########\n",
      "공부시간 : [[13]], 결과 : [0],[[0.50009391 0.49990609]]\n"
     ]
    }
   ],
   "source": [
    "### sklearn ### 으로 구현해보아요!\n",
    "\n",
    "# Logistic Regression Model을 생성해요!\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Training data set을 이용해서 학습\n",
    "model.fit(x_data,t_data.ravel())\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "predict_val = model.predict(study_hour)\n",
    "predict_proba = model.predict_proba(study_hour)\n",
    "print('####### sklearn 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {},{}'.format(study_hour,predict_val,predict_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mac/opt/anaconda3/envs/data_env/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "####### tensorflow 결과값 #########\n",
      "공부시간 : [13], 결과 : [[0.58152306]]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow를 이용한 구현\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(dtype=tf.float32)   # 독립변수가 1개인경우 shape명시하지 않아요! (x_data)\n",
    "T = tf.placeholder(dtype=tf.float32)   # (t_data)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = W * X + b  # matrix 곱연산 하지 않나요?? \n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    sess.run(train, feed_dict={X:x_data, T:t_data})\n",
    "\n",
    "\n",
    "study_hour = np.array([13])\n",
    "result = sess.run(H,feed_dict={X:study_hour})\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Variable Logistic Regression\n",
    "# 독립변수가 2개 이상인 Logistic Regression\n",
    "\n",
    "# 학습하는 데이터는 GRE(Graduate Record Examination)와\n",
    "# GPA(Grade Point Average) 성적 그리고\n",
    "# Rank(University Rating)에 대한 \n",
    "# 대학원 합격/ 불합격 정보\n",
    "\n",
    "# 내 성적 [600,   3.8,   1.] 의 결과 ??!\n",
    "# 첫번쨰 구현은 sklearn으로 하세요 !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## sklearn으로 구현한 결과 ##############\n",
      "[[600.    3.8   1. ]] [1] [[0.43740782 0.56259218]]\n",
      "W : [[-0.28196824]\n",
      " [ 0.42527026]\n",
      " [ 0.5707602 ]], b : [-1.2634385], loss : 0.6515757441520691\n",
      "W : [[-0.2705611 ]\n",
      " [ 0.43524867]\n",
      " [ 0.5608847 ]], b : [-1.255288], loss : 0.6502634286880493\n",
      "W : [[-0.25967857]\n",
      " [ 0.44466233]\n",
      " [ 0.5506254 ]], b : [-1.248038], loss : 0.6490464806556702\n",
      "W : [[-0.24926482]\n",
      " [ 0.4535667 ]\n",
      " [ 0.5400329 ]], b : [-1.2415514], loss : 0.6479062438011169\n",
      "W : [[-0.2392771 ]\n",
      " [ 0.46201462]\n",
      " [ 0.5291791 ]], b : [-1.2357485], loss : 0.6468302011489868\n",
      "W : [[-0.22967282]\n",
      " [ 0.4700514 ]\n",
      " [ 0.51805687]], b : [-1.230569], loss : 0.6458059549331665\n",
      "W : [[-0.22041437]\n",
      " [ 0.4777162 ]\n",
      " [ 0.50673884]], b : [-1.2259473], loss : 0.6448260545730591\n",
      "W : [[-0.2114698 ]\n",
      " [ 0.4850481 ]\n",
      " [ 0.49525797]], b : [-1.2218161], loss : 0.6438834071159363\n",
      "W : [[-0.20280738]\n",
      " [ 0.49207747]\n",
      " [ 0.4836256 ]], b : [-1.2181075], loss : 0.6429716944694519\n",
      "W : [[-0.19440031]\n",
      " [ 0.49883655]\n",
      " [ 0.4718757 ]], b : [-1.2147905], loss : 0.6420868635177612\n",
      "####### tensorflow 결과값 #########\n",
      "내 지원정보 : [[600.    3.8   1. ]], 결과 : [[0.29257238]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('~/notebook_dir/data/admission.csv')\n",
    "\n",
    "# 결측치 확인\n",
    "# df.isnull().sum() 결측치가 없다\n",
    "\n",
    "# 이상치를 확인해서 있으면 제거 !\n",
    "\n",
    "# fig = plt.figure() # 그림판을 만들어서 여러개의 데이터를 한꺼번에 표현\n",
    "\n",
    "# fig_admit = fig.add_subplot(1,4,1)\n",
    "# fig_gre = fig.add_subplot(1,4,2)\n",
    "# fig_gpa = fig.add_subplot(1,4,3)\n",
    "# fig_rank = fig.add_subplot(1,4,4)\n",
    "\n",
    "# fig_admit.boxplot(df['admit'])\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 확인했더니 이상치가 있어요 !\n",
    "# 이상치를 제거해요 !\n",
    "\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df.loc[~df[col].isin(outlier)]\n",
    "    \n",
    "    \n",
    "# print(df.shape)  # (382, 4)\n",
    "\n",
    "# Training Data set\n",
    "x_data = df.drop('admit',axis = 1, inplace=False).values # inplace=False : 원본삭제 x\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "# 정규화를 진행해야 해요 !!\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data) # for python, tensorflow\n",
    "\n",
    "# sklearn을 이용한 구현\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(x_data,t_data.ravel())\n",
    "print('########## sklearn으로 구현한 결과 ##############')\n",
    "my_score = np.array([[600, 3.8, 1]])\n",
    "predict_val = model.predict(my_score) # 0 or 1\n",
    "predict_proba = model.predict_proba(my_score) # (불합격할 확률, 합격할 확률)\n",
    "print(my_score, predict_val, predict_proba)\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype = tf.float32) # 독립션수의 데이터를 받기 위한 placeholder\n",
    "T = tf.placeholder(shape=[None,1], dtype = tf.float32) # 종속변수(label)의 데이터를 받기 위한 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name = 'Weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss],\n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    if step % 3000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val, b_val, loss_val))\n",
    "\n",
    "\n",
    "my_score = np.array([[600, 3.8, 1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "result = sess.run(H,feed_dict={X:scaled_my_score})\n",
    "\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('내 지원정보 : {}, 결과 : {}'.format(my_score,result))    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : [[ 0.07212371]\n",
      " [ 1.3021982 ]\n",
      " [-0.29034412]], b : [0.53358066], loss : 1.0369571447372437\n",
      "W : [[ 0.05273817]\n",
      " [ 1.2807405 ]\n",
      " [-0.3120066 ]], b : [0.49496257], loss : 1.0058391094207764\n",
      "W : [[ 0.03398602]\n",
      " [ 1.2597107 ]\n",
      " [-0.3330144 ]], b : [0.45751318], loss : 0.9765375852584839\n",
      "W : [[ 0.01586946]\n",
      " [ 1.2393259 ]\n",
      " [-0.35339853]], b : [0.42124665], loss : 0.9490499496459961\n",
      "W : [[-0.0016104 ]\n",
      " [ 1.219756  ]\n",
      " [-0.37315995]], b : [0.38614428], loss : 0.9233464598655701\n",
      "W : [[-0.01845615]\n",
      " [ 1.2005748 ]\n",
      " [-0.39227107]], b : [0.35220915], loss : 0.8992807269096375\n",
      "W : [[-0.03467251]\n",
      " [ 1.1823357 ]\n",
      " [-0.4107724 ]], b : [0.31943274], loss : 0.8768923878669739\n",
      "W : [[-0.05026515]\n",
      " [ 1.1645998 ]\n",
      " [-0.4286685 ]], b : [0.28779337], loss : 0.8560159802436829\n",
      "W : [[-0.06524297]\n",
      " [ 1.1474336 ]\n",
      " [-0.44594553]], b : [0.25727427], loss : 0.8365967273712158\n",
      "W : [[-0.07961664]\n",
      " [ 1.1311095 ]\n",
      " [-0.46263066]], b : [0.227861], loss : 0.8186072707176208\n",
      "Accuracy : 0.3560209572315216\n",
      "####### tensorflow 결과값 #########\n",
      "내 지원정보 : [[600.    3.8   1. ]], 결과 : [[0.7499723]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('~/notebook_dir/data/admission.csv')\n",
    "\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df.loc[~df[col].isin(outlier)]\n",
    "    \n",
    "    \n",
    "# Training Data set\n",
    "x_data = df.drop('admit',axis = 1, inplace=False).values # inplace=False : 원본삭제 x\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "# 정규화를 진행해야 해요 !!\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data) # for python, tensorflow\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype = tf.float32) # 독립션수의 데이터를 받기 위한 placeholder\n",
    "T = tf.placeholder(shape=[None,1], dtype = tf.float32) # 종속변수(label)의 데이터를 받기 위한 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name = 'Weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(90000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss],\n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    if step % 9000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val, b_val, loss_val))\n",
    "\n",
    "# 정확도 측정(Accuracy)\n",
    "\n",
    "predict = tf.cast(H >= 0.5, dtype = tf.float32) # True -> 1.0 , False -> 0.0\n",
    "correct = tf.equal(predict,T)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict = {X:norm_x_data,T:t_data})\n",
    "print('Accuracy : {}'.format(accuracy_val))\n",
    "# prediction\n",
    "my_score = np.array([[600, 3.8, 1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "result = sess.run(H,feed_dict={X:scaled_my_score})\n",
    "\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('내 지원정보 : {}, 결과 : {}'.format(my_score,result))    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
