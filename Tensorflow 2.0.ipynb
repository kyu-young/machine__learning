{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상환경 만들기 conda create -n data_env_tensorflow2 python=3.7 openssl\n",
    "# 필요한 모듈 설치\n",
    "# 텐서플로우 버전 확인해보기\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)  # 현재 설치된 TF 버전을 출력!\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 랜덤값을 하나 읽어와요!\n",
    "# numpy\n",
    "random1 = np.random.rand(2,2)\n",
    "print(random1)\n",
    "\n",
    "# tensorflow\n",
    "random2 = tf.random.normal([1], dtype=tf.float32)\n",
    "print(random2)   # tesor가 출력되요!\n",
    "# tf 1.x 버전에서는 node가 가지는 값을 얻어오려면 (node를 실행시키려면)\n",
    "# Session이 있어야 했어요!\n",
    "# tf 2.x 버전에서는 session없이 즉시 실행시킬 수 있어요!!(Eager Excution)\n",
    "print(random2.numpy())\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(10, dtype=tf.float32)\n",
    "b = tf.constant(20, dtype=tf.float32)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "print('c의 값은 : {}'.format(c.numpy()))\n",
    "\n",
    "d = 30.0\n",
    "\n",
    "tensor_d = tf.convert_to_tensor(d)\n",
    "\n",
    "print((c + tensor_d).numpy())\n",
    "import tensorflow as tf\n",
    "W = tf.Variable(tf.random.normal([1]),name ='weight')\n",
    "\n",
    "# 기존에는 tf.Variable()을 이용해서 변수를 만들면 사용하기 전에\n",
    "# 반드시 초기화를 진행해야 했어요!\n",
    "# sess.run(tf.global_variables_initializer()) \n",
    "# TF 2.0에서는 초기화를 안해도 되요!!\n",
    "print(W.numpy())\n",
    "# tensorflow graph에 입력을 주는 부분이 없어졌어요!\n",
    "# 기존에는 graph에게 데이터를 밀어넣기 위해서 placeholder를 이용했어요!\n",
    "# Eager Excution에 의해서 이제는 placeholder가 필요 없게 됐어요!\n",
    "# placsholder는 삭제되었습니다~\n",
    "\n",
    "# Lazy excution을 하지않아요! 이젠 Eager Excution을 수행해요!\n",
    "# tensorflow의 keras를 이용하여 Model을 생성해 보아요!\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# model = Sequential()  # keras model 생성\n",
    "\n",
    "model = tf.keras.models.Sequential()   # keras model 생성\n",
    "\n",
    "# model을 만들었으니 그 다음에는 layer를 만들어야 해요!\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(2,))) \n",
    "# input layer를 추가\n",
    "# 독립변수 2개 짜리 input layer\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "# output layer 추가\n",
    "# Dense(3) : 결과값의 개수\n",
    "# softmax를 이용해 결과값을 확률값으로 바꿔서\n",
    "# t를 onehot인코딩한 값과 비교!\n",
    "\n",
    "# modelcompile 과정\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# sklearn은 이미 만들어진 기성복같은!! 파라미터만 정하는것!\n",
    "# keras는 내가 원하는 방식으로 로직 자체를 바꿀수 있디!(모듈화!!)\n",
    "def my_loss:\n",
    "    pass\n",
    "\n",
    "# model 학습\n",
    "model.fit(x_data_train,\n",
    "          t_data_train,\n",
    "          epochs=100,\n",
    "          batch_size=100,\n",
    "          validation_split=0.3)\n",
    "# tensorflow 2.1을 이용해서 ozone 예제를 다시 구현해보아요!\n",
    "# Multiple Linear Regression\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential  # Model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Layer\n",
    "from tensorflow.keras.optimizers import SGD   # Optimizer\n",
    "from sklearn.preprocessing import MinMaxScaler   # Normalization\n",
    "from scipy import stats   # 이상치처리\n",
    "\n",
    "# Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "\n",
    "# 결측치 확인 및 처리\n",
    "# 일단 데이터가 충분히 많고 결측치가 적으면 삭제가 답이에요!\n",
    "# 하지만 일반적으로 결측치를 삭제하면 데이터가 너무 많이 유실되기 때문에\n",
    "# 다른 방식을 이용하게 되요!\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())  # 각 컬럼별 결측치 개수\n",
    "# KNN의 사용법에 대해서 알아보아요!\n",
    "# sklearn을 이용해서 알아보아요!\n",
    "\n",
    "# BMI예제를 이용해서 학습한 후 정확도를 특정해 보아요!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/bmi.csv')\n",
    "\n",
    "# data split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df[['height', 'weight']],\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                random_state=0)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_data_train_norm, t_data_train)\n",
    "print(model.score(x_data_test_norm, t_data_test)) # 생성된 모델 성능 확인\n",
    "\n",
    "# KNN을 이용한 분류\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(x_data_train_norm, t_data_train)\n",
    "print(knn_model.score(x_data_test_norm, t_data_test))   # 생성된 모델 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn의 결과 : [[38.75927452]]\n",
      "tensorflow의 결과 : [[38.626904]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats # 이상치 처리를 위해서 필요\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "import warnings\n",
    "\n",
    "# KNeighborsRegressor  : Regression 처리 할떄 사용 , 값을 찾을 때 사용\n",
    "# KNeighborsClassifier : 분류할 떄 사용 , 0 과 1 등등\n",
    "warnings.filterwarnings(action='ignore') # warning 출력을 하지 않아요\n",
    "\n",
    "# Raw Data Loading\n",
    "\n",
    "df = pd.read_csv('/Users/mac/notebook_dir/data/ozone.csv')\n",
    "\n",
    "training_data = df\n",
    "\n",
    "x_data = training_data[['Solar.R','Wind','Temp']]\n",
    "t_data = training_data['Ozone']\n",
    "\n",
    "# split을 하지 않는 이유는 linear regression을 하는거기 때문에 \n",
    "# 정확도 측정이 의미가 없기에 하지 않는다.\n",
    "\n",
    "# 결측치 확인\n",
    "# 1. 독립변수에 대한 결측치 처리부터 할게요 !\n",
    "# df.isnull().sum()\n",
    "# Ozone 이 종속변수, 나머지가 독립변수\n",
    "#   Solar.R 에 7개의 결측치가 있는데 median으로 처리할게요 !\n",
    "# nanmedian : nan을 제외하고 나머지 데이터의 데해 median을 구해라\n",
    "for col in x_data.columns:\n",
    "    col_median = np.nanmedian(x_data[col])\n",
    "    x_data[col].loc[x_data[col].isnull()] = col_median\n",
    "    \n",
    "# x_data.isnull().sum()\n",
    "\n",
    "# 2. 독립변수에 대한 이상치를 검출한 후 이 이상치는 mean 처리할게요 \n",
    "zscore = 1.8\n",
    "\n",
    "for col in x_data.columns:\n",
    "    outliers = x_data[col][np.abs(stats.zscore(x_data[col])) > zscore]\n",
    "#     print(outliers)\n",
    "    col_mean = np.mean(x_data.loc[~x_data[col].isin(outliers),col])\n",
    "    x_data.loc[x_data[col].isin(outliers),col] = col_mean\n",
    "#     print(col_mean)\n",
    "\n",
    "\n",
    "# 3. 정규화 진행해요 !\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_t = MinMaxScaler()\n",
    "\n",
    "# MinMax scaling하려면 2차원이 들어가야 하기 때문에 reshape 해줘야함\n",
    "scaler_x.fit(x_data.values)\n",
    "scaler_t.fit(t_data.values.reshape(-1,1))\n",
    "\n",
    "x_data_norm = scaler_x.transform(x_data)  # 2차원\n",
    "t_data_norm = scaler_t.transform(t_data.values.reshape(-1,1)).ravel()\n",
    "# 2차원으로 변경\n",
    "# 나중에 1차원으로 사용되야야 해서 그래서 ravel() 사용.\n",
    "    \n",
    "\n",
    "# 4. 종속변수(Ozone)에 대한  결측치는 KNN을 이용해서 예측값으로 imputation할 거에요 !\n",
    "#    학습에 사용될 x_data_train_norm , t_data_train_norm을 구해야 해요 !\n",
    "\n",
    "x_data_train_norm = x_data_norm[~np.isnan(t_data_norm)]\n",
    "t_data_train_norm = t_data_norm[~np.isnan(t_data_norm)]\n",
    "\n",
    "# KNN 모델 생성후 학습진행\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=2)\n",
    "knn_regressor.fit(x_data_train_norm,t_data_train_norm)\n",
    "\n",
    "# knn_predict \n",
    "knn_predict = knn_regressor.predict(x_data_norm[np.isnan(t_data_norm)])\n",
    "t_data_norm[np.isnan(t_data_norm)] = knn_predict\n",
    "\n",
    "# 최종 데이터를 생성했어요 !\n",
    "# x_data_norm\n",
    "# t_data_norm\n",
    "\n",
    "###############################################\n",
    "# 학습을 진행해 보아요 ! Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "test_data = [[310, 15, 80]] # 테스트 데이터 !! Ozone량을 예측해 보아요 !\n",
    "\n",
    "# sklearn\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(x_data_norm,t_data_norm)\n",
    "result = sklearn_model.predict(scaler_x.transform(test_data)).reshape(-1,1)\n",
    "\n",
    "scaled_result = scaler_t.inverse_transform(result)\n",
    "print('sklearn의 결과 : {}'.format(scaled_result))\n",
    "\n",
    "# Tensorflow 2.x \n",
    "# 모델생성\n",
    "keras_model = Sequential()\n",
    "# 레이어 추가\n",
    "keras_model.add(Flatten(input_shape=(3,))) # 안에 독립변수의 개수를 shape로 잡아줘야함\n",
    "# input Layer\n",
    "keras_model.add(Dense(1, activation='linear'))\n",
    "# 아웃풋의 개수가 들어감 , Ozone값 하나가 아웃풋임\n",
    "# W는 레이어 중간에 위치함, 하지만 우리는 코드상에 W를 표시하지는 않음\n",
    "# bias는 dense layer안에 숨어있다. ( 노드 안에 있음 )\n",
    "\n",
    "# compile\n",
    "keras_model.compile(optimizer=SGD(learning_rate=1e-2),\n",
    "                   loss = 'mse') # mse : 최소제곱법\n",
    "# 학습\n",
    "keras_model.fit(x_data_norm, t_data_norm,\n",
    "               epochs=5000,\n",
    "               verbose=0) # verbose : accuracy 있을떄 쓰면됨\n",
    "# predict\n",
    "result = keras_model.predict(scaler_x.transform(test_data)).reshape(-1,1)\n",
    "\n",
    "scaled_result = scaler_t.inverse_transform(result)\n",
    "print('tensorflow의 결과 : {}'.format(scaled_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression에 대해서 sklearn과 Tensorflow 2.x 구현을\n",
    "## 해볼거에요 !!\n",
    "## titanic(kaggle) => logistic 문제 (결측치가 다수 있어요 !)\n",
    "## feature engineering\n",
    "\n",
    "## 데이터를 완전히 준비하는 건 여러분들이 해보세요 !\n",
    "## 데이터 준비가 끝나면 같이 구현하는 부분만 해 보아요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex  Age  Embarked  Family\n",
       "0           0       3    0  2.0         0       1\n",
       "1           1       1    1  2.0         1       1\n",
       "2           1       3    1  2.0         0       0\n",
       "3           1       1    1  2.0         0       1\n",
       "4           0       3    0  2.0         0       0\n",
       "..        ...     ...  ...  ...       ...     ...\n",
       "886         0       2    0  2.0         0       0\n",
       "887         1       1    1  1.0         0       0\n",
       "888         0       3    1  2.0         0       3\n",
       "889         1       1    0  2.0         1       0\n",
       "890         0       3    0  2.0         2       0\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats # 이상치 처리를 위해서 필요\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "train = pd.read_csv('/Users/mac/notebook_dir/data/titanic/train.csv')\n",
    "test = pd.read_csv('/Users/mac/notebook_dir/data/titanic/test.csv')\n",
    "\n",
    "train.drop(['PassengerId','Ticket','Cabin','Name','Fare'],axis = 1, inplace = True)\n",
    "\n",
    "# 성별처리\n",
    "\n",
    "sex_mapping = { 'male' : 0, 'female' : 1}\n",
    "train['Sex'] = train['Sex'].map(sex_mapping)\n",
    "\n",
    "# 가족처리\n",
    "train['Family'] = train['SibSp'] + train['Parch']\n",
    "train.drop(['SibSp','Parch'], axis = 1, inplace = True)\n",
    "\n",
    "# Embarked 결측치 처리\n",
    "\n",
    "train['Embarked'] = train['Embarked'].fillna('Q')\n",
    "\n",
    "# Age에 대한 결측치 처리\n",
    "\n",
    "train['Age'] = train['Age'].fillna(train['Age'].mean())\n",
    "\n",
    "# Embarked 문자 => 숫자 처리\n",
    "\n",
    "embarked_mapping = {'S' : 0 , 'C' : 1, 'Q' : 2}\n",
    "train['Embarked'] = train['Embarked'].map(embarked_mapping)\n",
    "\n",
    "# Age에 대해서 Binning 처리(Numerical value -> categorical value)\n",
    "train.loc[train['Age'] < 8, 'Age'] = 0 # 아이는 0\n",
    "train.loc[(train['Age'] >= 8) & (train['Age'] < 20), 'Age'] = 1\n",
    "train.loc[(train['Age'] >= 20) & (train['Age'] < 65), 'Age'] = 2\n",
    "train.loc[train['Age'] >= 65, 'Age'] = 3\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? ㅛ\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         2\n",
      "         1.0       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# TF 2.0버전\n",
    "# Gate 연산으로 수행하는 Deep Learning으로 구현해보아요\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Training Data set\n",
    "x_data = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]], dtype = np.float32)\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Flatten(input_shape=(2,)))   # flatten은 생략하고 Dense에 추가가능\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(2,)))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-1),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_data,\n",
    "                   t_data,\n",
    "                   epochs=30000,\n",
    "                   verbose=0)\n",
    "predict_val = model.predict(x_data)\n",
    "\n",
    "result = tf.cast(predict_val >= 0.5, dtype=tf.float32).numpy().ravel()\n",
    "\n",
    "\n",
    "        \n",
    "# # 성능평가 ( Accuracy )\n",
    "# accuracy= tf.cast(H >= 0.5, dtype = tf.float32)\n",
    "# result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "print(classification_report(t_data.ravel(), result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_keys'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(history.history.keys()))\n",
    "\n",
    "# dict_keys(['loss','accuracy'])의 accuracy는\n",
    "# epoch당 train data를 이용한 acuuracy를 의미\n",
    "\n",
    "plt.plot(history.history['accuracy'], color = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# TF 2.0버전\n",
    "# Gate 연산으로 수행하는 Deep Learning으로 구현해보아요\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('~/notebook_dir/data/digit-recognizer/numtrain.csv')\n",
    "\n",
    "x_data = df.drop('label', axis=1)\n",
    "t_data = df['label']\n",
    "\n",
    "t_data\n",
    "# # Training Data set\n",
    "# x_data = np.array([[0,0],\n",
    "#                   [0,1],\n",
    "#                   [1,0],\n",
    "#                   [1,1]], dtype = np.float32)\n",
    "# t_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "# model = Sequential()\n",
    "# # model.add(Flatten(input_shape=(2,)))   # flatten은 생략하고 Dense에 추가가능\n",
    "# model.add(Dense(100, activation='sigmoid', input_shape=(2,)))\n",
    "# model.add(Dense(6, activation='sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer=SGD(learning_rate=1e-1),\n",
    "#              loss='binary_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(x_data,\n",
    "#                    t_data,\n",
    "#                    epochs=30000,\n",
    "#                    verbose=0)\n",
    "# predict_val = model.predict(x_data)\n",
    "\n",
    "# result = tf.cast(predict_val >= 0.5, dtype=tf.float32).numpy().ravel()\n",
    "\n",
    "\n",
    "        \n",
    "# # # 성능평가 ( Accuracy )\n",
    "# # accuracy= tf.cast(H >= 0.5, dtype = tf.float32)\n",
    "# # result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "# print(classification_report(t_data.ravel(), result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "# TF 2.0버전\n",
    "# Gate 연산으로 수행하는 Deep Learning으로 구현해보아요\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('~/notebook_dir/data/digit-recognizer/numtrain.csv')\n",
    "\n",
    "x_data = df.drop('label', axis=1)\n",
    "t_data = df['label']\n",
    "\n",
    "t_data\n",
    "# # Training Data set\n",
    "# x_data = np.array([[0,0],\n",
    "#                   [0,1],\n",
    "#                   [1,0],\n",
    "#                   [1,1]], dtype = np.float32)\n",
    "# t_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "# model = Sequential()\n",
    "# # model.add(Flatten(input_shape=(2,)))   # flatten은 생략하고 Dense에 추가가능\n",
    "# model.add(Dense(100, activation='sigmoid', input_shape=(2,)))\n",
    "# model.add(Dense(6, activation='sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(optimizer=SGD(learning_rate=1e-1),\n",
    "#              loss='binary_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(x_data,\n",
    "#                    t_data,\n",
    "#                    epochs=30000,\n",
    "#                    verbose=0)\n",
    "# predict_val = model.predict(x_data)\n",
    "\n",
    "# result = tf.cast(predict_val >= 0.5, dtype=tf.float32).numpy().ravel()\n",
    "\n",
    "\n",
    "        \n",
    "# # # 성능평가 ( Accuracy )\n",
    "# # accuracy= tf.cast(H >= 0.5, dtype = tf.float32)\n",
    "# # result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "# print(classification_report(t_data.ravel(), result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
