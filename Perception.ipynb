{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.880738377571106\n",
      "loss : 0.693333089351654\n",
      "loss : 0.6931636333465576\n",
      "loss : 0.6931487321853638\n",
      "loss : 0.6931473016738892\n",
      "loss : 0.6931471824645996\n",
      "loss : 0.6931471824645996\n",
      "loss : 0.6931471824645996\n",
      "loss : 0.6931471824645996\n",
      "loss : 0.6931471824645996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.50      0.40         2\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.25      0.20         4\n",
      "weighted avg       0.17      0.25      0.20         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 1.15버전으로\n",
    "# Gate 연산을 수행하는 Logistic Regression으로 구현해보아요\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Training Data set\n",
    "x_data = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]], dtype = np.float32)\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# placeholder \n",
    "\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "\n",
    "W = tf.Variable(tf.random.normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                             labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess  = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X:x_data,\n",
    "                                                    T:t_data})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss : {}'.format(loss_val))\n",
    "        \n",
    "# 성능평가 ( Accuracy )\n",
    "accuracy= tf.cast(H >= 0.5, dtype = tf.float32)\n",
    "result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "print(classification_report(t_data.ravel(), result.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "loss : 1.1478232145309448\n",
      "loss : 0.6937179565429688\n",
      "loss : 0.6775369048118591\n",
      "loss : 0.5920534133911133\n",
      "loss : 0.5317761898040771\n",
      "loss : 0.4170067608356476\n",
      "loss : 0.20501229166984558\n",
      "loss : 0.1168903112411499\n",
      "loss : 0.08102734386920929\n",
      "loss : 0.0620705671608448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         2\n",
      "         1.0       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# Gate 연산으로 수행하는 Deep Learning으로 구현해보아요\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Training Data set\n",
    "x_data = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [1,0],\n",
    "                  [1,1]], dtype = np.float32)\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# placeholder \n",
    "\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2,100]), name='weight2') \n",
    "# 몇개의 아웃풋이있나요?\n",
    "# 로지스틱이 몇개있나요? \n",
    "b2 = tf.Variable(tf.random.normal([100]), name='bias2')\n",
    "# 로지스틱이 100개있으니 \n",
    "# bias도 100개\n",
    "# shape만 변경해주면 됨 . 행렬 연산의 장점이다.\n",
    "layer2 = tf.sigmoid(tf.matmul(X, W2) + b2) \n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([100,6]), name='weight3') \n",
    "b3 = tf.Variable(tf.random.normal([6]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "# 지금까지는 히든 레이어는 2개\n",
    "\n",
    "\n",
    "\n",
    "# 최종 layer\n",
    "W4 = tf.Variable(tf.random.normal([6,1]), name='weight4') \n",
    "b4 = tf.Variable(tf.random.normal([1]), name='bias4')\n",
    "\n",
    "\n",
    "\n",
    "# Hypothesis\n",
    "\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                             labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess  = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X:x_data,\n",
    "                                                    T:t_data})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss : {}'.format(loss_val))\n",
    "        \n",
    "# 성능평가 ( Accuracy )\n",
    "accuracy= tf.cast(H >= 0.5, dtype = tf.float32)\n",
    "result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "print(classification_report(t_data.ravel(), result.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "\n",
    "# MNIST -> multinomial classification\n",
    "# 1.15버전 방식으로 단일 layer 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler # Normalizeration\n",
    "from sklearn.model_selection import train_test_split # train. test 데이터 분리\n",
    "from sklearn.model_selection import KFold # cross validation\n",
    "\n",
    "df = pd.read_csv('/Users/mac/notebook_dir/data/digit-recognizer/numtrain.csv')\n",
    "\n",
    "x_data= df.drop('label',axis=1,inplace=False).values\n",
    "t_data= df['label'].values\n",
    "\n",
    "x_data_train, x_data_test,t_data_train, t_data_test=\\\n",
    "train_test_split(x_data,t_data,test_size=0.3,random_state=0)\n",
    "\n",
    "# # Normalization 진행(MIN-MAX scaling)\n",
    "# scaler=MinMaxScaler()\n",
    "# scaler.fit(x_data_train) # 나중에 scaling을 하기 위한 정보를 scaler에게 세팅\n",
    "# x_data_train_norm=scaler.transform(x_data_train) # scaling 진행\n",
    "# x_data_test_norm=scaler.transform(x_data_test)   # scaling 진행\n",
    "\n",
    "# del x_data_test  # 에러방지용으로 삭제\n",
    "# del x_data_train # 에러방지용으로 삭제\n",
    "\n",
    "sess= tf.Session()\n",
    "# 우리가 사용할 label(t_data)를 one hot encoding 형태로 변환!!\n",
    "t_data_train_onehot=sess.run(tf.one_hot(t_data_train, depth=10))\n",
    "t_data_test_onehot=sess.run(tf.one_hot(t_data_test, depth=10))\n",
    "\n",
    "del t_data_train\n",
    "del t_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
